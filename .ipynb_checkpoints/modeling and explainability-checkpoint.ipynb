{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#!pip install --upgrade seaborn\n",
    "#sns.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem\n",
    "Agora, com nossos dados já tratados, seguiremos para a modelagem.\n",
    "\n",
    "Nesse arquivo, treinaremos nosso modelo para fazer a regressão do valor de venda do imóvel. Utilizaremos o algoritmo XGBoost, que é uma implementação otimizada de Gradient Boosting. O Gradient Boosting é um algoritmo de ensemble no qual treinamos classificadores de forma sequencial, em que cada classificador tenta corrigir os erros dos anteriores por meio da modalagem dos resíduos deixados por eles.\n",
    "\n",
    "* Utilizamos a técnica de otimização bayesiana para fazer o refinamento dos hiperparâmetros do xgboost.\n",
    "* O modelo final alcançou um erro absoluto médio de R\\\\$ 151.812,00 no conjunto de teste (pontos nunca vistos pelo modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/zap20201028/wrangled_data2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento das features\n",
    "Antes do treinamento, faremos um preprocessamento final das features. Vamos transformar as features categórias em dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.concat([pd.get_dummies(df.Street), df.drop('Street', axis=1)], axis=1)\n",
    "df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dummies.drop('Price', axis=1)\n",
    "y = df_dummies.Price\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "Nossos dados já estão no formato apropriado para serem inputados no modelo.\n",
    "\n",
    "Antes do treinamento, precisamos separar nosso conjunto de dados em conjunto de treinamento e teste. Faremos um split de 70/30%, respectivamente.\n",
    "\n",
    "Para fazer a comparação entre diferentes modelos e refinar os hiperparâmetros, não utilizaremos um conjunto de validação dedicado, mas sim a técnica de K-fold cross validation. Assim, não estaremos diminuindo ainda mais o tamanho de nosso conjunto de treinamento, que possui apenas 1228 exemplos. \n",
    "\n",
    "O conjunto de teste será usado uma única vez, ao final do projeto, para estimar a performance de nosso modelo em exemplos nunca vistos por ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "# formato apropriado dos dados para xgboost\n",
    "Dmatrix_train = xgb.DMatrix(data=X_train,label=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuição de Preços de imóvei no conjunto de teste (laranja) e de treinamento (azul). Perceba que a distribuição é aproximadamente a mesma, como desejado. Do contrário, estaríamos otimizando o modelo para uma distribuição diferente daquela que estamos interessados em acertar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_train)\n",
    "sns.distplot(y_test)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark model\n",
    "Vamos treinar um modelo com os parâmetros default do XGBoost para termos uma base de performance.\n",
    "\n",
    "Escolhi usar a métrica do erro absoluto médio (MAE) para avaliar a performance do modelo. Isso porque não desejo punir o algoritmo mais intensamente por resíduos maiores, como seria o caso do RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model = XGBRegressor()\n",
    "benchmark_model = benchmark_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avaliando métricas do modelo\n",
    "predictions = benchmark_model.predict(X_test)\n",
    "\n",
    "print('Resultado XGBoost')\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "\n",
    "sns.distplot((y_test-predictions),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso modelo de benchmark apresentou um MAE de R\\\\$ 155.623 no conjunto de teste. A seguir, refinaremos os hiperparâmetros para diminuir esse valor.\n",
    "\n",
    "Notamos também que os resíduos são aproximadamente normalmente distribuídos. Fico satisfeito com isso, pois o modelo não é tendencioso em prever valores sempre maiores, ou sempre menores, em comparação com os valores verdadeiros.\n",
    "\n",
    "## Refinando o modelo\n",
    "Agora que já treinamos nosso modelo de benchmark, vamos procurar os valores dos hiperparâmetros do xgboost que otimizam nossa métrica de avaliação, o MAE.\n",
    "\n",
    "A implementação do Gradient Boosting do XGBoosting possui vários parâmetros de treinamento, como por exemplo o learning_rate (coeficiente do resultado encontrado por cada árvore para o cálculo da previsão final) e max_depth (número máximo de camadas de cada árvore, que restringe a quantidade de splits que cada árvore faz). Esses parâmetros não são treináveis, i.e., devem ser informados manualmente ao algoritmo antes que ele realize o treinamento que irá determinar as especificações do modelo que fará a previsão de novos exemplos. Por isso, eles são chamados de hiperparâmetros.\n",
    "\n",
    "O processo de refinamento consiste em treinar diferentes modelos, que diferem pelos seus hiperparâmetros, e utilizar o conjunto de validação para verificar qual dos modelos faz a melhor previsão. Utilizaremos a técnica de k-fold cross validation, que separa nosso training set em k subconjuntos, treina o algoritmo em (k-1) desses subconjuntos e usa o subconjunto que ficou de fora para avaliar nossa métrica de avaliação, o MAE. Isso é repetido k vezes, cada vez com um subconjunto diferente de fora do treinamento. Ao final, fazemos a média dos k MAE medidos.\n",
    "\n",
    "As duas formas mais comuns de se testar diferentes conjuntos de hiperparâmetros são o \"grid search\" e o \"random search\". No primeiro, definimos um conjunto discreto de valores candidatos para cada hiperparâmetro, e treinamos modelos com todas as combinações possíveis entre os diferentes hiperparâmetros. No segundo, definimos intervalos de valores candidatos para cada hiperparâmetro, e selecionamos aleatoriamente, dentro de cada intervalo, os valores que serão utilizados em cada modelo.\n",
    "\n",
    "Uma terceira forma de fazer a busca por hiperparâmetros é o [Bayesian optimization](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). Nesse método, também sorteamos aleatoriamente valores para os hiperparâmetros. Mas, ao contrário do random search, onde a distribuição de probabilidades entre os valores possíveis é uniforme, o Bayesian optimization utiliza iterativamente os resultados que vão sendo obtidos para explorar mais intensamente os intervalos de valores mais promissores, para cada hiperparâmetro. Assim, a cada novo modelo treinado, é atualizada a distribuição de probabilidade dos valores associados a cada hiperparâmetro. A implementação do Bayesian optimization que utilizaremos aqui é a do pacote [hyperopt](https://github.com/hyperopt/hyperopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "#Espaço de hiperparâmetros a ser explorado:\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.6)),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 5.0),\n",
    "    'max_depth': hp.randint('max_depth', 4, 30),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', np.log(0.08), np.log(10)),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 2.0),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando o espaço de hiperparâmetros: sorteando um conjunto aleatório de hiperparâmetros\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "example = sample(space)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "\n",
    "#definindo a função que treina o modelo e calcula a métrica de avaliação (usando k-fold cv)\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"     \n",
    "    \n",
    "    #gerando o modelo\n",
    "    #model = XGBRegressor()\n",
    "    \n",
    "    #treinando o modelo\n",
    "    cv_results = xgb.cv(dtrain=Dmatrix_train, params=params, nfold=10,\n",
    "                    num_boost_round=1000,early_stopping_rounds=25, metrics=\"mae\", as_pandas=False, seed=101)\n",
    "    \n",
    "    #métrica de avaliação: 1 - ROC AUC\n",
    "    loss = cv_results['test-mae-mean'][-1]\n",
    "    \n",
    "    # dicionário com o resultado\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo 500 iterações da busca bayesiana\n",
    "\n",
    "# Algoritmo\n",
    "from hyperopt import tpe\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "\n",
    "# Objeto que registra o progresso\n",
    "from hyperopt import Trials\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# Função que faz a busca\n",
    "from hyperopt import fmin\n",
    "\n",
    "# Rodando a busca\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = 1, trials = bayes_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparâmetros do melhor modelo\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infos do melhor modelo\n",
    "bayes_trials.best_trial['result']#['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardando os hyperparâmetros do melhor modelo\n",
    "#params = bayes_trials.best_trial['result']['params']\n",
    "params = {'gamma': 2.199596107711117,\n",
    "  'learning_rate': 0.0974381627092206,\n",
    "  'max_depth': 29,\n",
    "  'min_child_weight': 3.2385338264269485,\n",
    "  'reg_alpha': 0.18472575899500998,\n",
    "  'reg_lambda': 0.38244758223682157}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando qual é o n_estimators ótimo para o melhor modelo\n",
    "cv_results = xgb.cv(dtrain=Dmatrix_train, params=params, nfold=10,\n",
    "                    num_boost_round=1000,early_stopping_rounds=25, metrics=\"mae\", as_pandas=True, seed=101)\n",
    "\n",
    "#best score\n",
    "cv_results.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando o melhor modelo\n",
    "final_model = XGBRegressor(\n",
    "        n_estimators=58,\n",
    "        learning_rate = params['learning_rate'],\n",
    "        gamma = params['gamma'],\n",
    "        max_depth = params['max_depth'],\n",
    "        min_child_weight = params['min_child_weight'],\n",
    "        reg_lambda = params['reg_lambda'],\n",
    "        reg_alpha = params['reg_alpha'],\n",
    "    )\n",
    "\n",
    "final_model = final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avaliando métricas do melhor modelo no conjunto de TREINAMENTO\n",
    "predictions = final_model.predict(X_train)\n",
    "\n",
    "print('Resultado XGBoost')\n",
    "print('MAE:', metrics.mean_absolute_error(y_train, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, predictions)))\n",
    "\n",
    "sns.distplot((y_train-predictions),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avaliando métricas do melhor modelo no conjunto de TESTE\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "print('Resultado XGBoost')\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "\n",
    "sns.distplot((y_test-predictions),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A busca bayesiana permitiu que refinássemos os hiperparâmetros, diminuindo o MAE de ___ (modelo de benchmark) para ___, no conjunto de teste.\n",
    "\n",
    "### Avaliando overfitting\n",
    "No conjunto de treinamento, obtivemos um MAE de cerca de R\\\\$ 22mil, enquanto que no conjunto de teste o MAE foi de cerca de R\\\\$ 150mil. Isso mostra que nosso modelo apresenta um grau de overfitting relativamente elevado. Ele está prevendo muito bem pontos do conjunto de treinamento, mas não está generalizando tão bem para pontos nunca vistos por ele no treinamento.\n",
    "\n",
    "Para minimizar o overfitting podemos otimizar certos hiperparâmetros do XGBoosting com que ainda não trabalhamos, como o subsample. Outra estratégia para combater o problema é aumentar o tamanho de nosso conjunto de treinamento.\n",
    "\n",
    "Numa próxima etapa do projeto tentaremos minimizar esse overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão para nosso cliente\n",
    "Com o modelo final já treinado, podemos fazer a previsão do valor de venda do imóvel de nosso cliente.\n",
    "\n",
    "O imóvel fica localizado na Avenida Oswaldo Cruz. Possui 102m², 4 quartos, 3 banheiros e 1 vaga de garagem. O condomínio é de R\\\\$ 2000, e o IPTU de R\\\\$ 250. Assim, nossas features assumem os seguintes valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_info = pd.Series([0,0,0,0,0,0,1,0,0,0,0,102, 4, 3, 1, 2000, 250], index=df_dummies.columns[:-1])\n",
    "\n",
    "x = pd.DataFrame(client_info).transpose()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fazendo a previsão para o apartamento de nosso cliente\n",
    "\n",
    "prediction = final_model.predict(x)\n",
    "\n",
    "print(f\"A previsão é de R$ {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos acima que a previsão de valor para o imóvel de nosso cliente é de R\\\\$ 1.183.002,00.\n",
    "\n",
    "É importante lembrar que esse valor foi encontrado com base nos valores dos anúncios de outros imóveis da região. É muito comum que sejam negociados descontos sobre os valores anunciados. Logo, não necessariamente o valor encontrado por nosso modelo é o valor de mercado real do imóvel.\n",
    "\n",
    "Vale salientar também que nosso modelo não incorporou características dos imóveis que são relevantes para os preços de venda, como estado de conservação do edifício e do imóvel, além da vista.\n",
    "\n",
    "# Explicabilidade do modelo\n",
    "Com o modelo já treinado e otimizado, agora nossa tarefa é entender e explicar quais são as decisões que o modelo toma para chegar a determinada previsão de valor.\n",
    "\n",
    "Usaremos o SHAP (SHapley Additive exPlanations), que é um approach baseado em teoria dos jogos para cumprir essa tarefa. Usaremos a implementação da biblioteca [shap](https://github.com/slundberg/shap) para Python.\n",
    "\n",
    "Primeiramente, vamos carregar a biblioteca, e encontrar os valores de shap para os exemplos de nosso conjunto de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "#calculando os valores de SHAP para cada feature de cada reserva\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe uma relação monotonamente crescente entre a soma dos valores de SHAP para todas as features de um imóvel, e a previsão que nosso model faz para o valor desse imóvel. Isso significa que quanto maior for o valor total de SHAP para um imóvel, maior é o valor de venda, previsto por nosso modelo. Em outras palavras, reservas que tem um valor total de SHAP alto também têm altos valores de venda. E reservas com valor total de SHAP baixo têm baixos valores de venda. O SHAP de uma certa feature, para um certo imóvel, informa a contribuição marginal que aquela feature tem na previsão de valor de venda.\n",
    "\n",
    "## Medindo Feature importances pelo SHAP\n",
    "Como valores de SHAP altos aumentam a previsão de valor de venda, e valores de SHAP baixos diminuem essa previsão, então a média dos módulos dos valores de SHAP para uma certa feature, ao longo de todo o nosso dataset, dá uma ideia de quão importante aquela feature é para que nosso modelo faça sua previsão.\n",
    "\n",
    "Essas importâncias relativas são mostradas no gráfico abaixo. Notamos que a área é, disparada, a feature mais importante para que nosso modelo faça a previsão de valor de venda de imóvel. Com muito menos importância, mas ainda sim importantes, temos o número de vagas de garagem. Em seguida, com igual importância, temos o valor de condomínio, IPTU, e o fato do imóvel estar (ou não) localizado na Avenida Rui Barbosa. A seguir, com um pouco menos importância, temos o número de banheiros e de dormitórios. As outras features informam se o imóvel está localizado em cada uma das ruas estudadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotando as feature importances finais\n",
    "shap.summary_plot(shap_values, X_train.columns, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impacto das features na previsão do modelo\n",
    "O gráfico abaixo dá uma visão geral das decisões que nosso modelo faz para chegar à previsão de valor de venda do imóvel. Cada linha representa uma feature diferente e deve ser lida separadamente. Perceba que as features são ordenadas da mais importante para a menos importante, de acordo com as feature importances mostradas acima.\n",
    "\n",
    "A primeira linha representa a área do imóvel. Temos uma correlação positiva: quanto maior é a área do imóvel, maior é a previsão de valor de venda encontrada por nosso modelo.\n",
    "\n",
    "A segunda linha representa quantas vagas de garagem possui o imóvel. Quando não há vagas (em azul), a previsão de valor é diminuída. Quando há 1 vaga (cluster roxo), a previsão é suavemente aumentada. Para mais vagas, temos maiores aumentos na previsão do modelo.\n",
    "\n",
    "A terceira e quarta linhas, de Condomínio e IPTU, respectivamente, apresentam comportamentos similares. Quando esses valores são mais baixos, a previsão do modelo tende a diminuir.\n",
    "\n",
    "A quinta linha, Rui Barbosa, mostra que nosso modelo atribui um valor premium a imóveis localizados nessa rua. Imóveis que não estão localizados na Rui Barbosa sofrem uma diminuição de previsão de nota, e imóveis que lá estão têm sua previsão aumentada.\n",
    "\n",
    "As linhas seis e sete mostram que a tendência é que a previsão de nota sejam maior quando o imóvel possui mais banheiros ou quartos, mas esse efeito é limitado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotando os valores de SHAP para cada feature de cada reserva\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area\n",
    "O gráfico abaixo mostra que a previsão do modelo tem relação aproximadamente linear com a área do imóvel, em especial para imóveis com áreas de até 140m²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Area', shap_values, X_train, interaction_index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vagas de Garagem\n",
    "O gráfico abaixo reafirma que imóveis com vagas de garagem tendem a ter maiores previsões de valor feitas por nosso modelo.\n",
    "\n",
    "Podemos observar um outro efeito interessante: caso o imóvel não possua vagas de garagem, a diminuição na previsão que nosso modelo faz é mais intensa para imóveis de maior área."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Parking Slots', shap_values, X_train, interaction_index=\"Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condomínio\n",
    "Valores baixos de condomínio tendem a fazer a previsão do nosso modelo diminuir.\n",
    "\n",
    "Em contrapartida, valores altos de condomínio não geram um aumento ou diminuição consistente de previsão do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Condominio', shap_values, X_train, interaction_index=\"Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rui Barbosa\n",
    "Nosso modelo associa um valor premium a apartamentos que estão localizados na Av. Rui Barbosa.\n",
    "\n",
    "O gráfico abaixo mostra um efeito interessante: apartamentos com grandes áreas sofrem uma penalização maior na previsão de nosso modelo por não estarem localizados na Rui Barbosa, se comparados a imóveis de menores áreas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Rui Barbosa', shap_values, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Banheiros\n",
    "Quanto maior o número de banheiros, maior a previsão de nosso modelo. Outro efeito interessante mostrado abaixo é que quando um imóvel possui 3 ou 4 banheiros, o aumento da previsão do modelo é mais intenso para imóveis de menor área. Em outras palavras, esses imóveis \"compensam\" sua menor área pela presença de um banheiro extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Baths', shap_values, X_train, interaction_index=\"Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quartos\n",
    "vemos abaixo que nosso modelo diminui fortemente a previsão de valor de imóveis que possuem apenas um quarto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Dorms', shap_values, X_train, interaction_index=\"Area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oswaldo Cruz\n",
    "Vemos abaixo que o modelo aumenta a previsão de nota quando o imóvel está localizado na Av. Oswaldo Cruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('Oswaldo Cruz', shap_values, X_train, interaction_index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
